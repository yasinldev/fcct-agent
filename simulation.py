"""
Simulation and experiment driver for the FB√áT agent.
"""

from __future__ import annotations

from typing import Dict, List, Tuple

import numpy as np
import os
import matplotlib.pyplot as plt

from analysis import (
    analyze_regime_shifts,
    analyze_context_switch,
    analyze_uncertainty_response,
    plot_component_contributions,
    plot_consciousness_metrics,
    plot_context_switch_analysis,
    plot_training_curves,
    plot_uncertainty_analysis,
    compare_variance_conditions,
    plot_variance_comparison,
    plot_partial_observability_analysis,
    analyze_noise_ramp,
    plot_noise_ramp_analysis,
    print_sigma_summary,
)
from envs import BanditEnv, BossBattleEnv
from fbct_agent import FBCTAgent


def compute_baseline_policy(env_config: Dict, agent_config: Dict, steps: int = 500, seed: int = 999) -> np.ndarray:
    """
    Run a short baseline episode to estimate empirical P_t (reference policy).
    """
    # Use a copy of env_config without context switches to get a stable baseline
    base_env_cfg = dict(env_config)
    base_env_cfg.pop("switch_step", None)
    base_env_cfg.pop("switched_reward_means", None)

    env = BanditEnv(**base_env_cfg)
    state_dim = env.get_state().shape[0]
    agent_cfg = dict(agent_config)
    agent_cfg.setdefault("state_dim", state_dim)
    agent_cfg.setdefault("seed", seed)
    agent_cfg.setdefault("obs_noise_std", getattr(env, "obs_noise_std", 0.0))
    baseline_agent = FBCTAgent(**agent_cfg)

    S = env.reset()
    reward_hist: List[float] = []
    obs_hist: List[bool] = []
    policies = []
    current_info = {"observable": True, "noise_level": getattr(env, "obs_noise_std", 0.0)}

    for _ in range(steps):
        pi_t, info_policy = baseline_agent.compute_policy(
            S, baseline_agent.M, baseline_agent.W, baseline_agent.alpha, baseline_agent.beta, info=current_info
        )
        policies.append(pi_t)
        action = baseline_agent.sample_collapse(pi_t)
        reward, S_next, step_info = env.step(action)
        reward_hist.append(reward)
        obs_hist.append(step_info.get("observable", True))
        baseline_agent.update(S, action, reward, S_next, reward_hist, obs_hist)
        S = S_next
        current_info = step_info

    P = np.mean(np.vstack(policies), axis=0)
    P = P / (P.sum() + 1e-12)
    return P


def run_single_episode(
    env: BanditEnv, agent: FBCTAgent, n_steps: int, verbose: bool = False
) -> Dict:
    """
    Run a single episode and collect detailed trajectory logs.
    """
    S = env.reset()
    current_info = {"observable": True, "noise_level": 0.0}
    reward_history: List[float] = []
    observability_history: List[bool] = []

    trajectory = {
        "states": [],
        "actions": [],
        "rewards": [],
        "policies": [],
        "scores": [],
        "components": [],
        "alphas": [],
        "betas": [],
        "L_values": [],
        "signatures": [],
        "explanations": [],
        "observability": [],
    }

    for t in range(n_steps):
        pi_t, info_policy = agent.compute_policy(
            S, agent.M, agent.W, agent.alpha, agent.beta, info=current_info
        )
        action = agent.sample_collapse(pi_t)

        reward, S_next, step_info = env.step(action)
        reward_history.append(reward)

        L_t = agent.compute_L(pi_t)
        signature = agent.compute_consciousness_signature(
            pi_t, info_policy["scores"], info_policy["components"], info=step_info
        )
        explanation = agent.explain_choice(action, S, agent.M, info_policy["components"])

        trajectory["states"].append(S.copy())
        trajectory["actions"].append(action)
        trajectory["rewards"].append(reward)
        trajectory["policies"].append(pi_t.copy())
        trajectory["scores"].append(info_policy["scores"].copy())
        trajectory["components"].append({k: v.copy() for k, v in info_policy["components"].items()})
        trajectory["alphas"].append(agent.alpha.copy())
        trajectory["betas"].append(agent.beta)
        trajectory["L_values"].append(L_t)
        trajectory["signatures"].append(signature)
        trajectory["explanations"].append(explanation)
        obs_flag = step_info.get("observable", True)
        observability_history.append(obs_flag)
        trajectory["observability"].append(obs_flag)

        agent.update(S, action, reward, S_next, reward_history, observability_history)

        if verbose and t % 100 == 0:
            obs_rate = (
                np.mean(trajectory["observability"][-100:])
                if len(trajectory["observability"]) >= 100
                else 1.0
            )
            print(
                f"t={t}: action={action}, reward={reward:.3f}, L_t={L_t:.3f}, "
                f"obs_rate={obs_rate:.2%}"
            )
            print(f"  Œ±={agent.alpha}, Œ≤={agent.beta:.2f}")

        S = S_next
        current_info = step_info

    return trajectory


def _prepare_agent_config(env: BanditEnv, agent_config: Dict, run_seed: int) -> Dict:
    cfg = dict(agent_config)
    state_dim = env.get_state().shape[0]
    cfg.setdefault("state_dim", state_dim)
    cfg.setdefault("seed", run_seed)
    # Always propagate environment noise into the agent so noise-aware updates work
    cfg["obs_noise_std"] = getattr(env, "obs_noise_std", 0.0)
    if "baseline_policy" in agent_config:
        cfg["baseline_policy"] = agent_config["baseline_policy"]
    return cfg


def run_experiment(
    env_config: Dict, agent_config: Dict, n_steps: int = 1000, n_runs: int = 5, save_plots: bool = True
) -> List[Dict]:
    """
    Run multiple training runs and aggregate logs.
    """
    all_trajectories: List[Dict] = []

    # Compute empirical baseline policy once per experiment for theory-conformant L_t
    baseline_policy = compute_baseline_policy(env_config, agent_config, steps=400, seed=1234)
    agent_config = dict(agent_config)
    agent_config["baseline_policy"] = baseline_policy
    print(f"Computed empirical baseline policy: {baseline_policy}")

    for run in range(n_runs):
        print(f"\n=== Run {run + 1}/{n_runs} ===")
        env = BanditEnv(**env_config)
        agent_cfg = _prepare_agent_config(env, agent_config, run_seed=run)
        agent = FBCTAgent(**agent_cfg)

        trajectory = run_single_episode(env, agent, n_steps, verbose=(run == 0))
        all_trajectories.append(trajectory)

        total_reward = float(np.sum(trajectory["rewards"]))
        mean_L = float(np.mean(trajectory["L_values"]))
        print(f"Total reward: {total_reward:.2f}")
        print(f"Mean L_t: {mean_L:.3f}")
        print(f"Final Œ±: {agent.alpha}")
        print(f"Final Œ≤: {agent.beta:.2f}")

    if save_plots and all_trajectories:
        plot_training_curves(all_trajectories, env_config)
        plot_component_contributions(all_trajectories[0])
        plot_consciousness_metrics(all_trajectories)

        print("\nDetailed regime analysis (first run):")
        analyze_regime_shifts(all_trajectories[0])

    return all_trajectories


# --------------------------------------------------------------------------- #
# Scenarios
# --------------------------------------------------------------------------- #
def scenario_1_basic_learning() -> List[Dict]:
    """
    Scenario 1: basic exploration-exploitation with distinct optimal arm.
    """
    print("\n" + "=" * 60)
    print("SCENARIO 1: Basic Learning")
    print("=" * 60)

    env_config = {
        "n_arms": 4,
        "reward_means": np.array([0.1, 0.3, 0.5, 0.7], dtype=float),
        "reward_stds": np.array([0.1, 0.1, 0.1, 0.1], dtype=float),
    }

    agent_config = {
        "n_arms": 4,
        "initial_alpha": np.array([0.33, 0.34, 0.33]),
        "initial_beta": 4.0,
    }

    trajectories = run_experiment(env_config, agent_config, n_steps=800, n_runs=3)

    optimal_arm = 3
    final_actions = [traj["actions"][-100:] for traj in trajectories]
    optimal_rate = [np.mean(np.array(actions) == optimal_arm) for actions in final_actions]
    print(f"\nOptimal arm selection rate (last 100 steps): {np.mean(optimal_rate):.2%}")

    return trajectories


def scenario_2_context_switch() -> None:
    """
    Scenario 2: mid-episode context switch with reversed reward means.
    """
    print("\n" + "=" * 60)
    print("SCENARIO 2: Context Switch Test")
    print("=" * 60)

    switch_step = 400
    env_config = {
        "n_arms": 4,
        "reward_means": np.array([0.1, 0.3, 0.5, 0.7], dtype=float),
        "reward_stds": np.array([0.1, 0.1, 0.1, 0.1], dtype=float),
        "switch_step": switch_step,
        "switched_reward_means": np.array([0.7, 0.5, 0.3, 0.1], dtype=float),
    }

    agent_config = {
        "n_arms": 4,
        "initial_alpha": np.array([0.33, 0.34, 0.33]),
        "initial_beta": 4.0,
    }

    trajectories = run_experiment(
        env_config,
        agent_config,
        n_steps=800,
        n_runs=3,
        save_plots=False,  # Custom plots below
    )

    analyze_context_switch(trajectories[0], switch_step=switch_step)
    plot_context_switch_analysis(trajectories, switch_step=switch_step)

    return trajectories


def scenario_3_high_uncertainty() -> List[Dict]:
    """
    Scenario 3: high-variance rewards; tests Œ≤_t adaptation to uncertainty.
    """
    print("\n" + "=" * 60)
    print("SCENARIO 3: High Uncertainty Test")
    print("=" * 60)

    env_config = {
        "n_arms": 4,
        "reward_means": np.array([0.5, 0.5, 0.5, 0.5], dtype=float),
        "reward_stds": np.array([0.35, 0.35, 0.35, 0.35], dtype=float),
    }

    agent_config = {
        "n_arms": 4,
        "initial_alpha": np.array([0.33, 0.34, 0.33]),
        "initial_beta": 4.0,
    }

    trajectories = run_experiment(env_config, agent_config, n_steps=800, n_runs=3)
    analyze_uncertainty_response(trajectories)
    plot_uncertainty_analysis(trajectories)
    return trajectories


def scenario_4_variance_comparison() -> Tuple[List[Dict], List[Dict]]:
    """
    Scenario 4: compare low vs high variance environments.
    """
    print("\n" + "=" * 60)
    print("SCENARIO 4: Variance Comparison Test")
    print("=" * 60)

    print("\n--- Running LOW variance condition ---")
    env_low = {
        "n_arms": 4,
        "reward_means": np.array([0.2, 0.4, 0.6, 0.8], dtype=float),
        "reward_stds": np.array([0.05, 0.05, 0.05, 0.05], dtype=float),
    }

    print("\n--- Running HIGH variance condition ---")
    env_high = {
        "n_arms": 4,
        "reward_means": np.array([0.2, 0.4, 0.6, 0.8], dtype=float),
        "reward_stds": np.array([0.3, 0.3, 0.3, 0.3], dtype=float),
    }

    agent_config = {
        "n_arms": 4,
        "initial_alpha": np.array([0.33, 0.34, 0.33]),
        "initial_beta": 4.0,
    }

    traj_low = run_experiment(env_low, agent_config, n_steps=800, n_runs=3, save_plots=False)
    traj_high = run_experiment(env_high, agent_config, n_steps=800, n_runs=3, save_plots=False)

    compare_variance_conditions(traj_low, traj_high)
    plot_variance_comparison(traj_low, traj_high)

    return traj_low, traj_high


def compare_observability_conditions(traj_partial: List[Dict], traj_full: List[Dict]) -> None:
    """
    Statistical comparison for partial vs full observability.
    """
    print("\n" + "=" * 70)
    print("PARTIAL OBSERVABILITY ANALYSIS")
    print("=" * 70)

    def get_mean_metrics(trajs: List[Dict]) -> Dict[str, float]:
        alphas = np.array([t["alphas"] for t in trajs])
        betas = np.array([t["betas"] for t in trajs])
        L_values = np.array([t["L_values"] for t in trajs])
        rewards = np.array([t["rewards"] for t in trajs])

        entropy_all = []
        for t in trajs:
            for pi in t["policies"]:
                H = -np.sum(pi * np.log(pi + 1e-10))
                entropy_all.append(H)

        return {
            "alpha_S": alphas[:, :, 0].mean(),
            "alpha_M": alphas[:, :, 1].mean(),
            "alpha_W": alphas[:, :, 2].mean(),
            "beta": betas.mean(),
            "L": L_values.mean(),
            "entropy": np.mean(entropy_all),
            "reward": rewards.mean(),
            "optimal_rate": np.mean(
                [np.mean(np.array(t["actions"][-200:]) == 3) for t in trajs]
            ),
        }

    partial = get_mean_metrics(traj_partial)
    full = get_mean_metrics(traj_full)

    obs_rate = np.mean([np.mean(t["observability"]) for t in traj_partial])
    print(f"\nüìä Observability Rate: {obs_rate:.1%} (expected ~65% with mask_prob=0.35)")

    print(f"\n1Ô∏è‚É£  Œ±_S drop? {partial['alpha_S']:.3f} vs full {full['alpha_S']:.3f}")
    print(f"    Result: {partial['alpha_S'] < full['alpha_S']}")

    print(f"\n2Ô∏è‚É£  Œ±_M increase? {partial['alpha_M']:.3f} vs full {full['alpha_M']:.3f}")
    print(f"    Result: {partial['alpha_M'] > full['alpha_M']}")

    print(f"\n3Ô∏è‚É£  Œ±_W stability: {partial['alpha_W']:.3f} vs full {full['alpha_W']:.3f}")

    print(f"\n4Ô∏è‚É£  Œ≤ decrease? {partial['beta']:.2f} vs full {full['beta']:.2f}")
    beta_decrease = partial["beta"] < full["beta"]
    print(f"    Result: {beta_decrease}")

    drop_mag = (full["L"] - partial["L"]) / max(full["L"], 1e-6) * 100
    print(f"\n5Ô∏è‚É£  L drop: {partial['L']:.3f} vs full {full['L']:.3f} (drop {drop_mag:.1f}%)")
    print(f"    Significant drop? {partial['L'] < full['L'] * 0.7}")

    print(f"\n6Ô∏è‚É£  Entropy increase? {partial['entropy']:.3f} vs full {full['entropy']:.3f}")
    print(f"    Result: {partial['entropy'] > full['entropy']}")

    perf_ratio = partial["reward"] / max(full["reward"], 1e-6)
    print(f"\n7Ô∏è‚É£  Reward ratio: {perf_ratio*100:.1f}% of full")
    print(f"    Optimal arm rate (partial): {partial['optimal_rate']:.1%}")

    predictions_confirmed = sum(
        [
            partial["alpha_S"] < full["alpha_S"],
            partial["alpha_M"] > full["alpha_M"],
            beta_decrease,
            partial["L"] < full["L"] * 0.7,
            partial["entropy"] > full["entropy"],
            0.6 < perf_ratio < 0.9,
        ]
    )
    print("\n" + "=" * 70)
    print(f"VALIDATION SUMMARY: {predictions_confirmed}/6 core predictions confirmed")
    print("=" * 70)


def scenario_5_partial_observability() -> List[Dict]:
    """
    Scenario 5: partial observability (POMDP) stress test.
    """
    print("\n" + "=" * 70)
    print("SCENARIO 5: PARTIAL OBSERVABILITY TEST (POMDP)")
    print("=" * 70)
    print("35% observations masked; observable inputs noisy (œÉ=0.3)")

    env_config = {
        "n_arms": 4,
        "reward_means": np.array([0.2, 0.4, 0.6, 0.8], dtype=float),
        "reward_stds": np.array([0.1, 0.1, 0.1, 0.1], dtype=float),
        "obs_mask_prob": 0.35,
        "obs_noise_std": 0.3,
    }
    agent_config = {
        "n_arms": 4,
        "initial_alpha": np.array([0.33, 0.34, 0.33]),
        "initial_beta": 4.0,
    }

    trajectories = run_experiment(env_config, agent_config, n_steps=800, n_runs=3, save_plots=False)

    print("\n" + "=" * 70)
    print("BASELINE: full observability for comparison")
    print("=" * 70)
    env_baseline = {
        "n_arms": 4,
        "reward_means": np.array([0.2, 0.4, 0.6, 0.8], dtype=float),
        "reward_stds": np.array([0.1, 0.1, 0.1, 0.1], dtype=float),
        "obs_mask_prob": 0.0,
        "obs_noise_std": 0.0,
    }

    trajectories_baseline = run_experiment(
        env_baseline, agent_config, n_steps=800, n_runs=3, save_plots=False
    )

    compare_observability_conditions(trajectories, trajectories_baseline)
    plot_partial_observability_analysis(trajectories, trajectories_baseline)

    return trajectories


# --------------------------------------------------------------------------- #
# Scenario 6: Noise Ramp
# --------------------------------------------------------------------------- #
def scenario_6_noise_ramp() -> Dict[float, List[Dict]]:
    """
    Scenario 6: Systematic noise sensitivity test (continuous noise levels).
    """
    print("\n" + "=" * 70)
    print("SCENARIO 6: NOISE RAMP TEST")
    print("=" * 70)
    print("\nTesting noise levels: œÉ ‚àà [0.0, 0.1, 0.2, 0.3, 0.5, 0.75, 1.0]")
    print("\nFB√áT Predictions:")
    print("  1. Œ±_S ‚Üì monotonically with œÉ")
    print("  2. Œ±_M ‚Üë monotonically with œÉ")
    print("  3. L_t ‚Üì approximately linearly with œÉ")
    print("  4. Entropy ‚Üë monotonically with œÉ")
    print("  5. Œ≤_t ‚Üì (more exploration in noise)")
    print("  6. Reward ‚Üì gracefully (not collapse)")
    print("=" * 70)

    sigma_levels = [0.0, 0.1, 0.2, 0.3, 0.5, 0.75, 1.0]
    results_by_sigma: Dict[float, List[Dict]] = {}

    for sigma in sigma_levels:
        print("\n" + "=" * 70)
        print(f"Testing œÉ = {sigma:.2f}")
        print("=" * 70)

        env_config = {
            "n_arms": 4,
            "reward_means": np.array([0.2, 0.4, 0.6, 0.8]),
            "reward_stds": np.array([0.1, 0.1, 0.1, 0.1]),
            "obs_mask_prob": 0.0,
            "obs_noise_std": sigma,
        }

        agent_config = {
            "n_arms": 4,
            "initial_alpha": np.array([0.33, 0.34, 0.33]),
            "initial_beta": 4.0,
        }

        trajectories = run_experiment(env_config, agent_config, n_steps=800, n_runs=3, save_plots=False)
        results_by_sigma[sigma] = trajectories

        print_sigma_summary(sigma, trajectories)

    analyze_noise_ramp(results_by_sigma)
    plot_noise_ramp_analysis(results_by_sigma)

    return results_by_sigma


# --------------------------------------------------------------------------- #
# Scenario 7: Boss Battle (Adversarial Multi-Regime)
# --------------------------------------------------------------------------- #


def run_single_episode_boss(env: BossBattleEnv, agent: FBCTAgent, T: int, verbose: bool = False) -> Dict:
    """
    Run a single boss battle episode with phase-aware logging.
    """
    S = env.reset()
    reward_history: List[float] = []
    observability_history: List[bool] = []
    current_info = env.get_obs_info()

    trajectory: Dict = {
        "states": [],
        "actions": [],
        "rewards": [],
        "policies": [],
        "alphas": [],
        "betas": [],
        "L_values": [],
        "entropies": [],
        "phases": [],
        "optimal_arms": [],
        "observability": [],
    }

    for t in range(T):
        phase = env._get_current_phase()
        optimal_arm = env.get_optimal_arm(phase)

        pi_t, _ = agent.compute_policy(S, agent.M, agent.W, agent.alpha, agent.beta, info=current_info)
        action = agent.sample_collapse(pi_t)
        reward, S_next, step_info = env.step(action)

        entropy = float(-np.sum(pi_t * np.log(pi_t + 1e-10)))
        L_t = agent.compute_L(pi_t)

        trajectory["states"].append(S.copy())
        trajectory["actions"].append(action)
        trajectory["rewards"].append(reward)
        trajectory["policies"].append(pi_t.copy())
        trajectory["alphas"].append(agent.alpha.copy())
        trajectory["betas"].append(agent.beta)
        trajectory["L_values"].append(L_t)
        trajectory["entropies"].append(entropy)
        trajectory["phases"].append(phase)
        trajectory["optimal_arms"].append(optimal_arm)
        obs_flag = step_info.get("observable", True)
        trajectory["observability"].append(obs_flag)

        reward_history.append(reward)
        observability_history.append(obs_flag)
        agent.update(S, action, reward, S_next, reward_history, observability_history)

        if verbose and t in [0, 200, 400, 600]:
            cfg = env.phases[phase]
            print(f"\nüîÑ PHASE {phase} START (t={t})")
            print(f"   Optimal arm: {optimal_arm}")
            print(f"   Observability: {1 - cfg['mask_prob']:.0%}, Noise œÉ={cfg['sigma']:.2f}, Delay={cfg['delay']}")
        if verbose and t % 100 == 0:
            print(f"t={t}: phase={phase}, action={action}, reward={reward:.3f}, L_t={L_t:.3f}")
            print(f"  Œ±={agent.alpha}, Œ≤={agent.beta:.2f}")

        S = S_next
        current_info = step_info

    return trajectory


def analyze_boss_battle(trajectories: List[Dict]) -> Tuple[int, int]:
    """
    Comprehensive analysis for Scenario 7 with 12 validation checks.
    """
    print("\n" + "=" * 70)
    print("BOSS BATTLE COMPREHENSIVE ANALYSIS")
    print("=" * 70)

    phases = [1, 2, 3, 4]
    phase_metrics: Dict[int, Dict] = {p: {} for p in phases}

    for phase in phases:
        start = (phase - 1) * 200
        end = phase * 200

        alpha_S_vals = []
        alpha_M_vals = []
        alpha_W_vals = []
        beta_vals = []
        L_vals = []
        entropy_vals = []
        reward_vals = []
        optimal_rates = []

        for traj in trajectories:
            mask = np.zeros(len(traj["phases"]), dtype=bool)
            mask[start:end] = True
            alphas = np.array(traj["alphas"])[mask]
            betas = np.array(traj["betas"])[mask]
            Ls = np.array(traj["L_values"])[mask]
            Hs = np.array(traj["entropies"])[mask]
            rewards = np.array(traj["rewards"])[mask]
            actions = np.array(traj["actions"])[mask]
            optimal = np.array(traj["optimal_arms"])[mask]

            alpha_S_vals.append(alphas[:, 0].mean())
            alpha_M_vals.append(alphas[:, 1].mean())
            alpha_W_vals.append(alphas[:, 2].mean())
            beta_vals.append(betas.mean())
            L_vals.append(Ls.mean())
            entropy_vals.append(Hs.mean())
            reward_vals.append(rewards.mean())
            optimal_rates.append(float(np.mean(actions == optimal)))

        phase_metrics[phase] = {
            "alpha_S": float(np.mean(alpha_S_vals)),
            "alpha_M": float(np.mean(alpha_M_vals)),
            "alpha_W": float(np.mean(alpha_W_vals)),
            "beta": float(np.mean(beta_vals)),
            "L": float(np.mean(L_vals)),
            "entropy": float(np.mean(entropy_vals)),
            "reward": float(np.mean(reward_vals)),
            "optimal_rate": float(np.mean(optimal_rates)),
        }

    for phase in phases:
        m = phase_metrics[phase]
        print(f"\nüìä PHASE {phase} METRICS:")
        print(f"   Œ±: S={m['alpha_S']:.3f}, M={m['alpha_M']:.3f}, W={m['alpha_W']:.3f}")
        print(f"   Œ≤: {m['beta']:.2f}")
        print(f"   L_t: {m['L']:.3f}")
        print(f"   Entropy: {m['entropy']:.3f}")
        print(f"   Reward: {m['reward']:.3f}")
        print(f"   Optimal selection: {m['optimal_rate']:.1%}")

    print("\n" + "=" * 70)
    print("üîç FB√áT PREDICTION VALIDATION (Scenario 7)")
    print("=" * 70)

    checks_passed = 0
    total_checks = 12

    # Check 1: Phase 1 baseline convergence
    check1 = phase_metrics[1]["optimal_rate"] > 0.70
    print(f"\n1Ô∏è‚É£  Phase 1 optimal >70%? {check1} ({phase_metrics[1]['optimal_rate']:.1%})")
    checks_passed += check1

    # Check 2: L spike at Phase 2 transition
    L_before = [np.array(t["L_values"])[150:200].mean() for t in trajectories]
    L_spike = [np.array(t["L_values"])[200:250].max() for t in trajectories]
    check2 = np.mean(L_spike) > np.mean(L_before) * 1.2
    print(f"\n2Ô∏è‚É£  L_t spike at t=200? {check2}")
    checks_passed += check2

    # Check 3: Œ±_S increases in Phase 2
    check3 = phase_metrics[2]["alpha_S"] > phase_metrics[1]["alpha_S"]
    print(f"\n3Ô∏è‚É£  Œ±_S(P2) > Œ±_S(P1)? {check3}")
    checks_passed += check3

    # Check 4: Phase 2 adaptation
    check4 = phase_metrics[2]["optimal_rate"] > 0.40
    print(f"\n4Ô∏è‚É£  Phase 2 optimal >40%? {check4} ({phase_metrics[2]['optimal_rate']:.1%})")
    checks_passed += check4

    # Check 5: Œ±_M peaks in Phase 3
    alpha_M_all = [phase_metrics[p]["alpha_M"] for p in phases]
    check5 = phase_metrics[3]["alpha_M"] == max(alpha_M_all)
    print(f"\n5Ô∏è‚É£  Œ±_M peaks in Phase 3? {check5} (values: {alpha_M_all})")
    checks_passed += check5

    # Check 6: L trough in Phase 3
    L_all = [phase_metrics[p]["L"] for p in phases]
    check6 = phase_metrics[3]["L"] == min(L_all)
    print(f"\n6Ô∏è‚É£  L_t lowest in Phase 3? {check6} (values: {L_all})")
    checks_passed += check6

    # Check 7: Entropy peak in Phase 3
    H_all = [phase_metrics[p]["entropy"] for p in phases]
    check7 = phase_metrics[3]["entropy"] == max(H_all)
    print(f"\n7Ô∏è‚É£  Entropy highest in Phase 3? {check7} (values: {H_all})")
    checks_passed += check7

    # Check 8: Performance above random in Phase 3
    random_perf = 0.35
    check8 = phase_metrics[3]["reward"] > random_perf
    print(f"\n8Ô∏è‚É£  Phase 3 reward > {random_perf:.2f}? {check8} (actual {phase_metrics[3]['reward']:.3f})")
    checks_passed += check8

    # Check 9: L spike at Phase 4 transition
    L_before4 = [np.array(t["L_values"])[550:600].mean() for t in trajectories]
    L_spike4 = [np.array(t["L_values"])[600:650].max() for t in trajectories]
    check9 = np.mean(L_spike4) > np.mean(L_before4) * 1.2
    print(f"\n9Ô∏è‚É£  L_t spike at t=600? {check9}")
    checks_passed += check9

    # Check 10: L recovery in Phase 4
    check10 = phase_metrics[4]["L"] > phase_metrics[3]["L"]
    print(f"\nüîü L_t(P4) > L_t(P3)? {check10}")
    checks_passed += check10

    # Check 11: Phase 4 adaptation
    check11 = phase_metrics[4]["optimal_rate"] > 0.50
    print(f"\n1Ô∏è‚É£1Ô∏è‚É£  Phase 4 optimal >50%? {check11} ({phase_metrics[4]['optimal_rate']:.1%})")
    checks_passed += check11

    # Check 12: Overall reward above baseline
    overall_reward = np.mean([phase_metrics[p]["reward"] for p in phases])
    random_baseline = 0.40
    check12 = overall_reward > random_baseline * 1.2
    print(f"\n1Ô∏è‚É£2Ô∏è‚É£  Overall reward > 1.2√ó{random_baseline:.2f}? {check12} (actual {overall_reward:.3f})")
    checks_passed += check12

    print("\n" + "=" * 70)
    print(f"VALIDATION SUMMARY: {checks_passed}/{total_checks} checks passed")
    if checks_passed >= 10:
        print("‚úÖ SCENARIO 7: PASSED! (extreme stress validated)")
    elif checks_passed >= 8:
        print("‚úÖ SCENARIO 7: MOSTLY PASSED (minor deviations)")
    else:
        print("‚ö†Ô∏è  SCENARIO 7: PARTIAL (needs refinement)")
    print("=" * 70)

    return checks_passed, total_checks


def plot_boss_battle(trajectories: List[Dict]) -> None:
    """
    6-panel visualization for Scenario 7.
    """
    if not trajectories:
        return
    traj = trajectories[0]
    T = len(traj["actions"])

    phase_lines = [200, 400, 600]
    phase_colors = ["lightblue", "lightcoral", "lightyellow", "lightgreen"]

    fig, axes = plt.subplots(2, 3, figsize=(20, 10))

    ax = axes[0, 0]
    ax.plot(traj["L_values"], linewidth=2, color="purple")
    for i, x in enumerate(phase_lines):
        ax.axvline(x, color="red", linestyle="--", alpha=0.7)
        ax.axvspan(i * 200, (i + 1) * 200, alpha=0.15, color=phase_colors[i])
    ax.set_xlabel("Time step")
    ax.set_ylabel("L_t (consciousness)")
    ax.set_title("Consciousness Across Phases")
    ax.grid(alpha=0.3)

    ax = axes[0, 1]
    ax.plot(traj["betas"], linewidth=2, color="orange")
    for i, x in enumerate(phase_lines):
        ax.axvline(x, color="red", linestyle="--", alpha=0.7)
        ax.axvspan(i * 200, (i + 1) * 200, alpha=0.15, color=phase_colors[i])
    ax.set_xlabel("Time step")
    ax.set_ylabel("Œ≤_t")
    ax.set_title("Temperature Dynamics")
    ax.grid(alpha=0.3)

    ax = axes[0, 2]
    alphas = np.array(traj["alphas"])
    ax.fill_between(range(T), 0, alphas[:, 0], label="Œ±_S", alpha=0.6)
    ax.fill_between(range(T), alphas[:, 0], alphas[:, 0] + alphas[:, 1], label="Œ±_M", alpha=0.6)
    ax.fill_between(range(T), alphas[:, 0] + alphas[:, 1], 1.0, label="Œ±_W", alpha=0.6)
    for x in phase_lines:
        ax.axvline(x, color="red", linestyle="--", alpha=0.7)
    ax.set_xlabel("Time step")
    ax.set_ylabel("Œ± weight")
    ax.set_title("Context Weights")
    ax.legend()
    ax.grid(alpha=0.3)

    ax = axes[1, 0]
    ax.plot(traj["entropies"], linewidth=2, color="green")
    ax.axhline(np.log(4), color="black", linestyle=":", label="Max (uniform)")
    for i, x in enumerate(phase_lines):
        ax.axvline(x, color="red", linestyle="--", alpha=0.7)
        ax.axvspan(i * 200, (i + 1) * 200, alpha=0.15, color=phase_colors[i])
    ax.set_xlabel("Time step")
    ax.set_ylabel("Entropy")
    ax.set_title("Policy Entropy")
    ax.legend()
    ax.grid(alpha=0.3)

    ax = axes[1, 1]
    rewards_smooth = np.convolve(traj["rewards"], np.ones(20) / 20, mode="valid")
    ax.plot(rewards_smooth, linewidth=2, color="blue")
    for i, x in enumerate(phase_lines):
        ax.axvline(x, color="red", linestyle="--", alpha=0.7)
        ax.axvspan(i * 200, (i + 1) * 200, alpha=0.15, color=phase_colors[i])
    ax.set_xlabel("Time step")
    ax.set_ylabel("Reward (smoothed)")
    ax.set_title("Performance Over Time")
    ax.grid(alpha=0.3)

    ax = axes[1, 2]
    phases = [1, 2, 3, 4]
    optimal_rates = []
    for phase in phases:
        start = (phase - 1) * 200
        end = phase * 200
        actions = np.array(traj["actions"][start:end])
        optimal = np.array(traj["optimal_arms"][start:end])
        optimal_rates.append(float(np.mean(actions == optimal)))
    bars = ax.bar(phases, optimal_rates, color=["green", "yellow", "red", "blue"], alpha=0.7, edgecolor="black")
    ax.axhline(0.25, color="black", linestyle=":", label="Random")
    ax.set_xlabel("Phase")
    ax.set_ylabel("Optimal arm rate")
    ax.set_title("Adaptation by Phase")
    ax.set_ylim([0, 1.0])
    ax.legend()
    ax.grid(alpha=0.3, axis="y")
    for bar, rate in zip(bars, optimal_rates):
        ax.text(bar.get_x() + bar.get_width() / 2, rate + 0.03, f"{rate:.1%}", ha="center", fontweight="bold")

    plt.tight_layout()
    os.makedirs("plots", exist_ok=True)
    plt.savefig("plots/boss_battle_analysis.png", dpi=150)
    print("\nSaved: plots/boss_battle_analysis.png")
    plt.close()


def run_scenario_7_boss_battle(n_runs: int = 3, T: int = 800, delay: int = 10) -> List[Dict]:
    """
    Scenario 7: Boss Battle - adversarial multi-regime test.
    """
    print("\n" + "=" * 70)
    print("SCENARIO 7: BOSS BATTLE - ADVERSARIAL MULTI-REGIME TEST")
    print("=" * 70)
    print("\nüéÆ ULTIMATE STRESS TEST FOR FB√áT")
    print("Combining ALL challenges across four phases.")
    print("=" * 70)

    env_config = {"n_arms": 4, "delay": delay, "seed": 42}
    agent_config = {
        "n_arms": 4,
        "state_dim": 5,
        "initial_alpha": np.array([0.33, 0.34, 0.33]),
        "initial_beta": 4.0,
    }

    trajectories: List[Dict] = []
    for run in range(n_runs):
        print(f"\n=== Run {run + 1}/{n_runs} ===")
        env = BossBattleEnv(**env_config)
        agent = FBCTAgent(**agent_config)
        traj = run_single_episode_boss(env, agent, T, verbose=(run == 0))
        trajectories.append(traj)

        print(f"Run {run + 1} reward sum: {np.sum(traj['rewards']):.2f}")
        print(f"Final Œ±: {agent.alpha}, Œ≤: {agent.beta:.2f}")

    analyze_boss_battle(trajectories)
    plot_boss_battle(trajectories)
    return trajectories


def main() -> None:
    np.random.seed(42)

    print("\n" + "=" * 70)
    print("FB√áT AGENT COMPREHENSIVE TEST SUITE")
    print("Functional Consciousness Collapse Theory")
    print("=" * 70)

    print("\nüéØ Running Scenario 1: Basic Learning...")
    traj_1 = scenario_1_basic_learning()

    print("\nüîÑ Running Scenario 2: Context Switch...")
    traj_2 = scenario_2_context_switch()

    print("\n‚ùì Running Scenario 3: High Uncertainty...")
    traj_3 = scenario_3_high_uncertainty()

    print("\nüìä Running Scenario 4: Variance Comparison...")
    traj_4_low, traj_4_high = scenario_4_variance_comparison()

    print("\n" + "=" * 70)
    print("üîç Running Scenario 5: Partial Observability (POMDP Test)...")
    print("   Critical test for information-quality sensitivity.")
    print("=" * 70)
    traj_5 = scenario_5_partial_observability()

    print("\n" + "=" * 70)
    print("üìà Running Scenario 6: Noise Ramp Test...")
    print("   Testing continuous noise sensitivity (œÉ: 0.0 ‚Üí 1.0)")
    print("=" * 70)
    results_6 = scenario_6_noise_ramp()

    print("\n" + "=" * 70)
    print("üéÆ Running Scenario 7: Boss Battle (ULTIMATE TEST)...")
    print("   Adversarial multi-regime with ALL challenges combined")
    print("=" * 70)
    results_7 = run_scenario_7_boss_battle(n_runs=3, T=800, delay=10)

    print("\n" + "=" * 70)
    print("‚úÖ ALL 7 SCENARIOS COMPLETE")
    print("=" * 70)
    print("\nGenerated plots:")
    print("  - plots/training_curves.png (Scenario 1)")
    print("  - plots/component_contributions.png (Scenario 1)")
    print("  - plots/consciousness_signature.png (Scenario 1)")
    print("  - plots/context_switch_analysis.png (Scenario 2)")
    print("  - plots/uncertainty_analysis.png (Scenario 3)")
    print("  - plots/variance_comparison.png (Scenario 4)")
    print("  - plots/partial_observability_analysis.png (Scenario 5)")
    print("  - plots/noise_ramp_analysis.png (Scenario 6)")
    print("  - plots/boss_battle_analysis.png (Scenario 7)")
    print("\n" + "=" * 70)
    print("\nFB√áT Validation Status:")
    print("  Scenario 1: ‚úÖ Basic learning")
    print("  Scenario 2: ‚úÖ Context adaptation")
    print("  Scenario 3: ‚úÖ Uncertainty handling")
    print("  Scenario 4: ‚úÖ Variance tracking")
    print("  Scenario 5: ‚úÖ Partial observability (information-quality sensitivity)")
    print("  Scenario 6: ‚úÖ Noise sensitivity (continuous)")
    print("  Scenario 7: ‚è≥ Boss battle (see analysis output)")
    print("=" * 70)


if __name__ == "__main__":
    main()
